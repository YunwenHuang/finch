{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Word Index Built\n",
      "==> Sequence Padded\n",
      "Epoch 1/200 | Batch 0/77 | train_loss: 2.698 | test_loss: 2.694\n",
      "Epoch 1/200 | Batch 50/77 | train_loss: 2.312 | test_loss: 2.284\n",
      "Epoch 2/200 | Batch 0/77 | train_loss: 2.051 | test_loss: 2.007\n",
      "Epoch 2/200 | Batch 50/77 | train_loss: 1.801 | test_loss: 1.710\n",
      "Epoch 3/200 | Batch 0/77 | train_loss: 1.651 | test_loss: 1.591\n",
      "Epoch 3/200 | Batch 50/77 | train_loss: 1.418 | test_loss: 1.314\n",
      "Epoch 4/200 | Batch 0/77 | train_loss: 1.113 | test_loss: 1.086\n",
      "Epoch 4/200 | Batch 50/77 | train_loss: 0.923 | test_loss: 0.842\n",
      "Epoch 5/200 | Batch 0/77 | train_loss: 0.777 | test_loss: 0.758\n",
      "Epoch 5/200 | Batch 50/77 | train_loss: 0.705 | test_loss: 0.658\n",
      "Epoch 6/200 | Batch 0/77 | train_loss: 0.631 | test_loss: 0.623\n",
      "Epoch 6/200 | Batch 50/77 | train_loss: 0.610 | test_loss: 0.577\n",
      "Epoch 7/200 | Batch 0/77 | train_loss: 0.555 | test_loss: 0.553\n",
      "Epoch 7/200 | Batch 50/77 | train_loss: 0.548 | test_loss: 0.519\n",
      "Epoch 8/200 | Batch 0/77 | train_loss: 0.503 | test_loss: 0.499\n",
      "Epoch 8/200 | Batch 50/77 | train_loss: 0.498 | test_loss: 0.463\n",
      "Epoch 9/200 | Batch 0/77 | train_loss: 0.457 | test_loss: 0.445\n",
      "Epoch 9/200 | Batch 50/77 | train_loss: 0.456 | test_loss: 0.428\n",
      "Epoch 10/200 | Batch 0/77 | train_loss: 0.423 | test_loss: 0.408\n",
      "Epoch 10/200 | Batch 50/77 | train_loss: 0.420 | test_loss: 0.393\n",
      "Epoch 11/200 | Batch 0/77 | train_loss: 0.396 | test_loss: 0.382\n",
      "Epoch 11/200 | Batch 50/77 | train_loss: 0.394 | test_loss: 0.368\n",
      "Epoch 12/200 | Batch 0/77 | train_loss: 0.376 | test_loss: 0.363\n",
      "Epoch 12/200 | Batch 50/77 | train_loss: 0.376 | test_loss: 0.350\n",
      "Epoch 13/200 | Batch 0/77 | train_loss: 0.368 | test_loss: 0.350\n",
      "Epoch 13/200 | Batch 50/77 | train_loss: 0.354 | test_loss: 0.332\n",
      "Epoch 14/200 | Batch 0/77 | train_loss: 0.350 | test_loss: 0.330\n",
      "Epoch 14/200 | Batch 50/77 | train_loss: 0.339 | test_loss: 0.315\n",
      "Epoch 15/200 | Batch 0/77 | train_loss: 0.338 | test_loss: 0.318\n",
      "Epoch 15/200 | Batch 50/77 | train_loss: 0.325 | test_loss: 0.303\n",
      "Epoch 16/200 | Batch 0/77 | train_loss: 0.325 | test_loss: 0.307\n",
      "Epoch 16/200 | Batch 50/77 | train_loss: 0.312 | test_loss: 0.295\n",
      "Epoch 17/200 | Batch 0/77 | train_loss: 0.317 | test_loss: 0.297\n",
      "Epoch 17/200 | Batch 50/77 | train_loss: 0.301 | test_loss: 0.285\n",
      "Epoch 18/200 | Batch 0/77 | train_loss: 0.305 | test_loss: 0.286\n",
      "Epoch 18/200 | Batch 50/77 | train_loss: 0.290 | test_loss: 0.274\n",
      "Epoch 19/200 | Batch 0/77 | train_loss: 0.296 | test_loss: 0.277\n",
      "Epoch 19/200 | Batch 50/77 | train_loss: 0.281 | test_loss: 0.265\n",
      "Epoch 20/200 | Batch 0/77 | train_loss: 0.290 | test_loss: 0.270\n",
      "Epoch 20/200 | Batch 50/77 | train_loss: 0.271 | test_loss: 0.257\n",
      "Epoch 21/200 | Batch 0/77 | train_loss: 0.285 | test_loss: 0.263\n",
      "Epoch 21/200 | Batch 50/77 | train_loss: 0.266 | test_loss: 0.250\n",
      "Epoch 22/200 | Batch 0/77 | train_loss: 0.280 | test_loss: 0.253\n",
      "Epoch 22/200 | Batch 50/77 | train_loss: 0.255 | test_loss: 0.256\n",
      "Epoch 23/200 | Batch 0/77 | train_loss: 0.264 | test_loss: 0.251\n",
      "Epoch 23/200 | Batch 50/77 | train_loss: 0.247 | test_loss: 0.239\n",
      "Epoch 24/200 | Batch 0/77 | train_loss: 0.254 | test_loss: 0.239\n",
      "Epoch 24/200 | Batch 50/77 | train_loss: 0.248 | test_loss: 0.235\n",
      "Epoch 25/200 | Batch 0/77 | train_loss: 0.243 | test_loss: 0.234\n",
      "Epoch 25/200 | Batch 50/77 | train_loss: 0.241 | test_loss: 0.246\n",
      "Epoch 26/200 | Batch 0/77 | train_loss: 0.236 | test_loss: 0.230\n",
      "Epoch 26/200 | Batch 50/77 | train_loss: 0.230 | test_loss: 0.253\n",
      "Epoch 27/200 | Batch 0/77 | train_loss: 0.229 | test_loss: 0.229\n",
      "Epoch 27/200 | Batch 50/77 | train_loss: 0.226 | test_loss: 0.236\n",
      "Epoch 28/200 | Batch 0/77 | train_loss: 0.224 | test_loss: 0.222\n",
      "Epoch 28/200 | Batch 50/77 | train_loss: 0.219 | test_loss: 0.215\n",
      "Epoch 29/200 | Batch 0/77 | train_loss: 0.217 | test_loss: 0.218\n",
      "Epoch 29/200 | Batch 50/77 | train_loss: 0.212 | test_loss: 0.210\n",
      "Epoch 30/200 | Batch 0/77 | train_loss: 0.213 | test_loss: 0.213\n",
      "Epoch 30/200 | Batch 50/77 | train_loss: 0.209 | test_loss: 0.207\n",
      "Epoch 31/200 | Batch 0/77 | train_loss: 0.209 | test_loss: 0.211\n",
      "Epoch 31/200 | Batch 50/77 | train_loss: 0.205 | test_loss: 0.204\n",
      "Epoch 32/200 | Batch 0/77 | train_loss: 0.205 | test_loss: 0.209\n",
      "Epoch 32/200 | Batch 50/77 | train_loss: 0.201 | test_loss: 0.200\n",
      "Epoch 33/200 | Batch 0/77 | train_loss: 0.202 | test_loss: 0.206\n",
      "Epoch 33/200 | Batch 50/77 | train_loss: 0.198 | test_loss: 0.197\n",
      "Epoch 34/200 | Batch 0/77 | train_loss: 0.198 | test_loss: 0.202\n",
      "Epoch 34/200 | Batch 50/77 | train_loss: 0.196 | test_loss: 0.194\n",
      "Epoch 35/200 | Batch 0/77 | train_loss: 0.195 | test_loss: 0.201\n",
      "Epoch 35/200 | Batch 50/77 | train_loss: 0.194 | test_loss: 0.191\n",
      "Epoch 36/200 | Batch 0/77 | train_loss: 0.192 | test_loss: 0.199\n",
      "Epoch 36/200 | Batch 50/77 | train_loss: 0.196 | test_loss: 0.190\n",
      "Epoch 37/200 | Batch 0/77 | train_loss: 0.189 | test_loss: 0.196\n",
      "Epoch 37/200 | Batch 50/77 | train_loss: 0.192 | test_loss: 0.189\n",
      "Epoch 38/200 | Batch 0/77 | train_loss: 0.186 | test_loss: 0.189\n",
      "Epoch 38/200 | Batch 50/77 | train_loss: 0.185 | test_loss: 0.190\n",
      "Epoch 39/200 | Batch 0/77 | train_loss: 0.188 | test_loss: 0.185\n",
      "Epoch 39/200 | Batch 50/77 | train_loss: 0.179 | test_loss: 0.189\n",
      "Epoch 40/200 | Batch 0/77 | train_loss: 0.187 | test_loss: 0.184\n",
      "Epoch 40/200 | Batch 50/77 | train_loss: 0.177 | test_loss: 0.183\n",
      "Epoch 41/200 | Batch 0/77 | train_loss: 0.181 | test_loss: 0.185\n",
      "Epoch 41/200 | Batch 50/77 | train_loss: 0.175 | test_loss: 0.179\n",
      "Epoch 42/200 | Batch 0/77 | train_loss: 0.178 | test_loss: 0.181\n",
      "Epoch 42/200 | Batch 50/77 | train_loss: 0.172 | test_loss: 0.177\n",
      "Epoch 43/200 | Batch 0/77 | train_loss: 0.175 | test_loss: 0.178\n",
      "Epoch 43/200 | Batch 50/77 | train_loss: 0.170 | test_loss: 0.176\n",
      "Epoch 44/200 | Batch 0/77 | train_loss: 0.173 | test_loss: 0.175\n",
      "Epoch 44/200 | Batch 50/77 | train_loss: 0.168 | test_loss: 0.174\n",
      "Epoch 45/200 | Batch 0/77 | train_loss: 0.171 | test_loss: 0.173\n",
      "Epoch 45/200 | Batch 50/77 | train_loss: 0.167 | test_loss: 0.172\n",
      "Epoch 46/200 | Batch 0/77 | train_loss: 0.169 | test_loss: 0.171\n",
      "Epoch 46/200 | Batch 50/77 | train_loss: 0.164 | test_loss: 0.171\n",
      "Epoch 47/200 | Batch 0/77 | train_loss: 0.167 | test_loss: 0.168\n",
      "Epoch 47/200 | Batch 50/77 | train_loss: 0.162 | test_loss: 0.170\n",
      "Epoch 48/200 | Batch 0/77 | train_loss: 0.165 | test_loss: 0.166\n",
      "Epoch 48/200 | Batch 50/77 | train_loss: 0.161 | test_loss: 0.169\n",
      "Epoch 49/200 | Batch 0/77 | train_loss: 0.163 | test_loss: 0.163\n",
      "Epoch 49/200 | Batch 50/77 | train_loss: 0.160 | test_loss: 0.167\n",
      "Epoch 50/200 | Batch 0/77 | train_loss: 0.161 | test_loss: 0.161\n",
      "Epoch 50/200 | Batch 50/77 | train_loss: 0.159 | test_loss: 0.165\n",
      "Epoch 51/200 | Batch 0/77 | train_loss: 0.159 | test_loss: 0.159\n",
      "Epoch 51/200 | Batch 50/77 | train_loss: 0.158 | test_loss: 0.163\n",
      "Epoch 52/200 | Batch 0/77 | train_loss: 0.157 | test_loss: 0.158\n",
      "Epoch 52/200 | Batch 50/77 | train_loss: 0.156 | test_loss: 0.162\n",
      "Epoch 53/200 | Batch 0/77 | train_loss: 0.156 | test_loss: 0.156\n",
      "Epoch 53/200 | Batch 50/77 | train_loss: 0.153 | test_loss: 0.160\n",
      "Epoch 54/200 | Batch 0/77 | train_loss: 0.155 | test_loss: 0.154\n",
      "Epoch 54/200 | Batch 50/77 | train_loss: 0.153 | test_loss: 0.157\n",
      "Epoch 55/200 | Batch 0/77 | train_loss: 0.154 | test_loss: 0.153\n",
      "Epoch 55/200 | Batch 50/77 | train_loss: 0.151 | test_loss: 0.156\n",
      "Epoch 56/200 | Batch 0/77 | train_loss: 0.154 | test_loss: 0.153\n",
      "Epoch 56/200 | Batch 50/77 | train_loss: 0.149 | test_loss: 0.154\n",
      "Epoch 57/200 | Batch 0/77 | train_loss: 0.151 | test_loss: 0.151\n",
      "Epoch 57/200 | Batch 50/77 | train_loss: 0.149 | test_loss: 0.153\n",
      "Epoch 58/200 | Batch 0/77 | train_loss: 0.148 | test_loss: 0.149\n",
      "Epoch 58/200 | Batch 50/77 | train_loss: 0.147 | test_loss: 0.152\n",
      "Epoch 59/200 | Batch 0/77 | train_loss: 0.146 | test_loss: 0.148\n",
      "Epoch 59/200 | Batch 50/77 | train_loss: 0.147 | test_loss: 0.150\n",
      "Epoch 60/200 | Batch 0/77 | train_loss: 0.145 | test_loss: 0.150\n",
      "Epoch 60/200 | Batch 50/77 | train_loss: 0.151 | test_loss: 0.148\n",
      "Epoch 61/200 | Batch 0/77 | train_loss: 0.141 | test_loss: 0.145\n",
      "Epoch 61/200 | Batch 50/77 | train_loss: 0.148 | test_loss: 0.149\n",
      "Epoch 62/200 | Batch 0/77 | train_loss: 0.142 | test_loss: 0.144\n",
      "Epoch 62/200 | Batch 50/77 | train_loss: 0.144 | test_loss: 0.152\n",
      "Epoch 63/200 | Batch 0/77 | train_loss: 0.141 | test_loss: 0.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200 | Batch 50/77 | train_loss: 0.158 | test_loss: 0.151\n",
      "Epoch 64/200 | Batch 0/77 | train_loss: 0.153 | test_loss: 0.147\n",
      "Epoch 64/200 | Batch 50/77 | train_loss: 0.139 | test_loss: 0.147\n",
      "Epoch 65/200 | Batch 0/77 | train_loss: 0.139 | test_loss: 0.143\n",
      "Epoch 65/200 | Batch 50/77 | train_loss: 0.137 | test_loss: 0.145\n",
      "Epoch 66/200 | Batch 0/77 | train_loss: 0.137 | test_loss: 0.141\n",
      "Epoch 66/200 | Batch 50/77 | train_loss: 0.135 | test_loss: 0.145\n",
      "Epoch 67/200 | Batch 0/77 | train_loss: 0.135 | test_loss: 0.139\n",
      "Epoch 67/200 | Batch 50/77 | train_loss: 0.134 | test_loss: 0.143\n",
      "Epoch 68/200 | Batch 0/77 | train_loss: 0.133 | test_loss: 0.138\n",
      "Epoch 68/200 | Batch 50/77 | train_loss: 0.132 | test_loss: 0.143\n",
      "Epoch 69/200 | Batch 0/77 | train_loss: 0.131 | test_loss: 0.138\n",
      "Epoch 69/200 | Batch 50/77 | train_loss: 0.131 | test_loss: 0.142\n",
      "Epoch 70/200 | Batch 0/77 | train_loss: 0.129 | test_loss: 0.138\n",
      "Epoch 70/200 | Batch 50/77 | train_loss: 0.130 | test_loss: 0.139\n",
      "Epoch 71/200 | Batch 0/77 | train_loss: 0.129 | test_loss: 0.135\n",
      "Epoch 71/200 | Batch 50/77 | train_loss: 0.129 | test_loss: 0.138\n",
      "Epoch 72/200 | Batch 0/77 | train_loss: 0.128 | test_loss: 0.134\n",
      "Epoch 72/200 | Batch 50/77 | train_loss: 0.128 | test_loss: 0.136\n",
      "Epoch 73/200 | Batch 0/77 | train_loss: 0.128 | test_loss: 0.132\n",
      "Epoch 73/200 | Batch 50/77 | train_loss: 0.127 | test_loss: 0.135\n",
      "Epoch 74/200 | Batch 0/77 | train_loss: 0.127 | test_loss: 0.131\n",
      "Epoch 74/200 | Batch 50/77 | train_loss: 0.126 | test_loss: 0.134\n",
      "Epoch 75/200 | Batch 0/77 | train_loss: 0.126 | test_loss: 0.130\n",
      "Epoch 75/200 | Batch 50/77 | train_loss: 0.125 | test_loss: 0.133\n",
      "Epoch 76/200 | Batch 0/77 | train_loss: 0.124 | test_loss: 0.130\n",
      "Epoch 76/200 | Batch 50/77 | train_loss: 0.124 | test_loss: 0.132\n",
      "Epoch 77/200 | Batch 0/77 | train_loss: 0.123 | test_loss: 0.129\n",
      "Epoch 77/200 | Batch 50/77 | train_loss: 0.122 | test_loss: 0.132\n",
      "Epoch 78/200 | Batch 0/77 | train_loss: 0.122 | test_loss: 0.129\n",
      "Epoch 78/200 | Batch 50/77 | train_loss: 0.121 | test_loss: 0.131\n",
      "Epoch 79/200 | Batch 0/77 | train_loss: 0.121 | test_loss: 0.128\n",
      "Epoch 79/200 | Batch 50/77 | train_loss: 0.120 | test_loss: 0.130\n",
      "Epoch 80/200 | Batch 0/77 | train_loss: 0.119 | test_loss: 0.127\n",
      "Epoch 80/200 | Batch 50/77 | train_loss: 0.120 | test_loss: 0.130\n",
      "Epoch 81/200 | Batch 0/77 | train_loss: 0.117 | test_loss: 0.127\n",
      "Epoch 81/200 | Batch 50/77 | train_loss: 0.119 | test_loss: 0.129\n",
      "Epoch 82/200 | Batch 0/77 | train_loss: 0.116 | test_loss: 0.127\n",
      "Epoch 82/200 | Batch 50/77 | train_loss: 0.118 | test_loss: 0.128\n",
      "Epoch 83/200 | Batch 0/77 | train_loss: 0.115 | test_loss: 0.126\n",
      "Epoch 83/200 | Batch 50/77 | train_loss: 0.118 | test_loss: 0.128\n",
      "Epoch 84/200 | Batch 0/77 | train_loss: 0.114 | test_loss: 0.126\n",
      "Epoch 84/200 | Batch 50/77 | train_loss: 0.117 | test_loss: 0.129\n",
      "Epoch 85/200 | Batch 0/77 | train_loss: 0.112 | test_loss: 0.126\n",
      "Epoch 85/200 | Batch 50/77 | train_loss: 0.117 | test_loss: 0.133\n",
      "Epoch 86/200 | Batch 0/77 | train_loss: 0.111 | test_loss: 0.125\n",
      "Epoch 86/200 | Batch 50/77 | train_loss: 0.115 | test_loss: 0.135\n",
      "Epoch 87/200 | Batch 0/77 | train_loss: 0.111 | test_loss: 0.125\n",
      "Epoch 87/200 | Batch 50/77 | train_loss: 0.116 | test_loss: 0.131\n",
      "Epoch 88/200 | Batch 0/77 | train_loss: 0.115 | test_loss: 0.128\n",
      "Epoch 88/200 | Batch 50/77 | train_loss: 0.114 | test_loss: 0.127\n",
      "Epoch 89/200 | Batch 0/77 | train_loss: 0.112 | test_loss: 0.127\n",
      "Epoch 89/200 | Batch 50/77 | train_loss: 0.134 | test_loss: 0.160\n",
      "Epoch 90/200 | Batch 0/77 | train_loss: 0.135 | test_loss: 0.130\n",
      "Epoch 90/200 | Batch 50/77 | train_loss: 0.116 | test_loss: 0.125\n",
      "Epoch 91/200 | Batch 0/77 | train_loss: 0.106 | test_loss: 0.129\n",
      "Epoch 91/200 | Batch 50/77 | train_loss: 0.122 | test_loss: 0.123\n",
      "Epoch 92/200 | Batch 0/77 | train_loss: 0.111 | test_loss: 0.134\n",
      "Epoch 92/200 | Batch 50/77 | train_loss: 0.119 | test_loss: 0.125\n",
      "Epoch 93/200 | Batch 0/77 | train_loss: 0.110 | test_loss: 0.128\n",
      "Epoch 93/200 | Batch 50/77 | train_loss: 0.146 | test_loss: 0.128\n",
      "Epoch 94/200 | Batch 0/77 | train_loss: 0.105 | test_loss: 0.127\n",
      "Epoch 94/200 | Batch 50/77 | train_loss: 0.109 | test_loss: 0.120\n",
      "Epoch 95/200 | Batch 0/77 | train_loss: 0.107 | test_loss: 0.124\n",
      "Epoch 95/200 | Batch 50/77 | train_loss: 0.107 | test_loss: 0.119\n",
      "Epoch 96/200 | Batch 0/77 | train_loss: 0.102 | test_loss: 0.123\n",
      "Epoch 96/200 | Batch 50/77 | train_loss: 0.107 | test_loss: 0.119\n",
      "Epoch 97/200 | Batch 0/77 | train_loss: 0.100 | test_loss: 0.122\n",
      "Epoch 97/200 | Batch 50/77 | train_loss: 0.107 | test_loss: 0.119\n",
      "Epoch 98/200 | Batch 0/77 | train_loss: 0.099 | test_loss: 0.120\n",
      "Epoch 98/200 | Batch 50/77 | train_loss: 0.107 | test_loss: 0.118\n",
      "Epoch 99/200 | Batch 0/77 | train_loss: 0.098 | test_loss: 0.120\n",
      "Epoch 99/200 | Batch 50/77 | train_loss: 0.106 | test_loss: 0.118\n",
      "Epoch 100/200 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.120\n",
      "Epoch 100/200 | Batch 50/77 | train_loss: 0.111 | test_loss: 0.118\n",
      "Epoch 101/200 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.120\n",
      "Epoch 101/200 | Batch 50/77 | train_loss: 0.105 | test_loss: 0.117\n",
      "Epoch 102/200 | Batch 0/77 | train_loss: 0.095 | test_loss: 0.119\n",
      "Epoch 102/200 | Batch 50/77 | train_loss: 0.105 | test_loss: 0.119\n",
      "Epoch 103/200 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.119\n",
      "Epoch 103/200 | Batch 50/77 | train_loss: 0.103 | test_loss: 0.117\n",
      "Epoch 104/200 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.120\n",
      "Epoch 104/200 | Batch 50/77 | train_loss: 0.102 | test_loss: 0.116\n",
      "Epoch 105/200 | Batch 0/77 | train_loss: 0.093 | test_loss: 0.121\n",
      "Epoch 105/200 | Batch 50/77 | train_loss: 0.102 | test_loss: 0.117\n",
      "Epoch 106/200 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.117\n",
      "Epoch 106/200 | Batch 50/77 | train_loss: 0.101 | test_loss: 0.114\n",
      "Epoch 107/200 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.119\n",
      "Epoch 107/200 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.115\n",
      "Epoch 108/200 | Batch 0/77 | train_loss: 0.091 | test_loss: 0.117\n",
      "Epoch 108/200 | Batch 50/77 | train_loss: 0.099 | test_loss: 0.114\n",
      "Epoch 109/200 | Batch 0/77 | train_loss: 0.090 | test_loss: 0.117\n",
      "Epoch 109/200 | Batch 50/77 | train_loss: 0.101 | test_loss: 0.113\n",
      "Epoch 110/200 | Batch 0/77 | train_loss: 0.090 | test_loss: 0.116\n",
      "Epoch 110/200 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.114\n",
      "Epoch 111/200 | Batch 0/77 | train_loss: 0.102 | test_loss: 0.114\n",
      "Epoch 111/200 | Batch 50/77 | train_loss: 0.149 | test_loss: 0.136\n",
      "Epoch 112/200 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.122\n",
      "Epoch 112/200 | Batch 50/77 | train_loss: 0.098 | test_loss: 0.115\n",
      "Epoch 113/200 | Batch 0/77 | train_loss: 0.096 | test_loss: 0.117\n",
      "Epoch 113/200 | Batch 50/77 | train_loss: 0.097 | test_loss: 0.115\n",
      "Epoch 114/200 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.115\n",
      "Epoch 114/200 | Batch 50/77 | train_loss: 0.095 | test_loss: 0.111\n",
      "Epoch 115/200 | Batch 0/77 | train_loss: 0.096 | test_loss: 0.113\n",
      "Epoch 115/200 | Batch 50/77 | train_loss: 0.098 | test_loss: 0.114\n",
      "Epoch 116/200 | Batch 0/77 | train_loss: 0.095 | test_loss: 0.110\n",
      "Epoch 116/200 | Batch 50/77 | train_loss: 0.094 | test_loss: 0.111\n",
      "Epoch 117/200 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.109\n",
      "Epoch 117/200 | Batch 50/77 | train_loss: 0.093 | test_loss: 0.110\n",
      "Epoch 118/200 | Batch 0/77 | train_loss: 0.090 | test_loss: 0.110\n",
      "Epoch 118/200 | Batch 50/77 | train_loss: 0.092 | test_loss: 0.112\n",
      "Epoch 119/200 | Batch 0/77 | train_loss: 0.088 | test_loss: 0.109\n",
      "Epoch 119/200 | Batch 50/77 | train_loss: 0.092 | test_loss: 0.112\n",
      "Epoch 120/200 | Batch 0/77 | train_loss: 0.088 | test_loss: 0.109\n",
      "Epoch 120/200 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.111\n",
      "Epoch 121/200 | Batch 0/77 | train_loss: 0.086 | test_loss: 0.108\n",
      "Epoch 121/200 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.110\n",
      "Epoch 122/200 | Batch 0/77 | train_loss: 0.085 | test_loss: 0.108\n",
      "Epoch 122/200 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.111\n",
      "Epoch 123/200 | Batch 0/77 | train_loss: 0.083 | test_loss: 0.109\n",
      "Epoch 123/200 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.111\n",
      "Epoch 124/200 | Batch 0/77 | train_loss: 0.083 | test_loss: 0.108\n",
      "Epoch 124/200 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.111\n",
      "Epoch 125/200 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.108\n",
      "Epoch 125/200 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.109\n",
      "Epoch 126/200 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.109\n",
      "Epoch 127/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.108\n",
      "Epoch 127/200 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.118\n",
      "Epoch 128/200 | Batch 0/77 | train_loss: 0.087 | test_loss: 0.118\n",
      "Epoch 128/200 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.113\n",
      "Epoch 129/200 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.117\n",
      "Epoch 129/200 | Batch 50/77 | train_loss: 0.095 | test_loss: 0.109\n",
      "Epoch 130/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.110\n",
      "Epoch 130/200 | Batch 50/77 | train_loss: 0.088 | test_loss: 0.110\n",
      "Epoch 131/200 | Batch 0/77 | train_loss: 0.084 | test_loss: 0.108\n",
      "Epoch 131/200 | Batch 50/77 | train_loss: 0.102 | test_loss: 0.196\n",
      "Epoch 132/200 | Batch 0/77 | train_loss: 0.087 | test_loss: 0.114\n",
      "Epoch 132/200 | Batch 50/77 | train_loss: 0.093 | test_loss: 0.114\n",
      "Epoch 133/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.109\n",
      "Epoch 133/200 | Batch 50/77 | train_loss: 0.089 | test_loss: 0.107\n",
      "Epoch 134/200 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.113\n",
      "Epoch 134/200 | Batch 50/77 | train_loss: 0.085 | test_loss: 0.106\n",
      "Epoch 135/200 | Batch 0/77 | train_loss: 0.080 | test_loss: 0.109\n",
      "Epoch 135/200 | Batch 50/77 | train_loss: 0.084 | test_loss: 0.106\n",
      "Epoch 136/200 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.108\n",
      "Epoch 136/200 | Batch 50/77 | train_loss: 0.083 | test_loss: 0.105\n",
      "Epoch 137/200 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.106\n",
      "Epoch 137/200 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.105\n",
      "Epoch 138/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.106\n",
      "Epoch 138/200 | Batch 50/77 | train_loss: 0.084 | test_loss: 0.105\n",
      "Epoch 139/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.105\n",
      "Epoch 139/200 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.105\n",
      "Epoch 140/200 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.105\n",
      "Epoch 140/200 | Batch 50/77 | train_loss: 0.081 | test_loss: 0.106\n",
      "Epoch 141/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.108\n",
      "Epoch 141/200 | Batch 50/77 | train_loss: 0.086 | test_loss: 0.105\n",
      "Epoch 142/200 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.109\n",
      "Epoch 142/200 | Batch 50/77 | train_loss: 0.083 | test_loss: 0.105\n",
      "Epoch 143/200 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.103\n",
      "Epoch 143/200 | Batch 50/77 | train_loss: 0.094 | test_loss: 0.110\n",
      "Epoch 144/200 | Batch 0/77 | train_loss: 0.077 | test_loss: 0.105\n",
      "Epoch 144/200 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.107\n",
      "Epoch 145/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.102\n",
      "Epoch 145/200 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.105\n",
      "Epoch 146/200 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.106\n",
      "Epoch 146/200 | Batch 50/77 | train_loss: 0.089 | test_loss: 0.138\n",
      "Epoch 147/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.105\n",
      "Epoch 147/200 | Batch 50/77 | train_loss: 0.179 | test_loss: 0.119\n",
      "Epoch 148/200 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.141\n",
      "Epoch 148/200 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.107\n",
      "Epoch 149/200 | Batch 0/77 | train_loss: 0.080 | test_loss: 0.103\n",
      "Epoch 149/200 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.106\n",
      "Epoch 150/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.101\n",
      "Epoch 150/200 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.107\n",
      "Epoch 151/200 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.100\n",
      "Epoch 151/200 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.107\n",
      "Epoch 152/200 | Batch 0/77 | train_loss: 0.080 | test_loss: 0.099\n",
      "Epoch 152/200 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.106\n",
      "Epoch 153/200 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.099\n",
      "Epoch 153/200 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.106\n",
      "Epoch 154/200 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.100\n",
      "Epoch 154/200 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.105\n",
      "Epoch 155/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.099\n",
      "Epoch 155/200 | Batch 50/77 | train_loss: 0.076 | test_loss: 0.104\n",
      "Epoch 156/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.098\n",
      "Epoch 156/200 | Batch 50/77 | train_loss: 0.076 | test_loss: 0.104\n",
      "Epoch 157/200 | Batch 0/77 | train_loss: 0.077 | test_loss: 0.099\n",
      "Epoch 157/200 | Batch 50/77 | train_loss: 0.076 | test_loss: 0.104\n",
      "Epoch 158/200 | Batch 0/77 | train_loss: 0.075 | test_loss: 0.099\n",
      "Epoch 158/200 | Batch 50/77 | train_loss: 0.075 | test_loss: 0.104\n",
      "Epoch 159/200 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.097\n",
      "Epoch 159/200 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.105\n",
      "Epoch 160/200 | Batch 0/77 | train_loss: 0.073 | test_loss: 0.099\n",
      "Epoch 160/200 | Batch 50/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 161/200 | Batch 0/77 | train_loss: 0.075 | test_loss: 0.097\n",
      "Epoch 161/200 | Batch 50/77 | train_loss: 0.074 | test_loss: 0.103\n",
      "Epoch 162/200 | Batch 0/77 | train_loss: 0.073 | test_loss: 0.096\n",
      "Epoch 162/200 | Batch 50/77 | train_loss: 0.073 | test_loss: 0.104\n",
      "Epoch 163/200 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.098\n",
      "Epoch 163/200 | Batch 50/77 | train_loss: 0.073 | test_loss: 0.107\n",
      "Epoch 164/200 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.096\n",
      "Epoch 164/200 | Batch 50/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 165/200 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.099\n",
      "Epoch 165/200 | Batch 50/77 | train_loss: 0.074 | test_loss: 0.102\n",
      "Epoch 166/200 | Batch 0/77 | train_loss: 0.073 | test_loss: 0.097\n",
      "Epoch 166/200 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.104\n",
      "Epoch 167/200 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.099\n",
      "Epoch 167/200 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.102\n",
      "Epoch 168/200 | Batch 0/77 | train_loss: 0.071 | test_loss: 0.097\n",
      "Epoch 168/200 | Batch 50/77 | train_loss: 0.070 | test_loss: 0.102\n",
      "Epoch 169/200 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.099\n",
      "Epoch 169/200 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.101\n",
      "Epoch 170/200 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.100\n",
      "Epoch 170/200 | Batch 50/77 | train_loss: 0.123 | test_loss: 0.144\n",
      "Epoch 171/200 | Batch 0/77 | train_loss: 0.096 | test_loss: 0.110\n",
      "Epoch 171/200 | Batch 50/77 | train_loss: 0.075 | test_loss: 0.105\n",
      "Epoch 172/200 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.101\n",
      "Epoch 172/200 | Batch 50/77 | train_loss: 0.073 | test_loss: 0.103\n",
      "Epoch 173/200 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.097\n",
      "Epoch 173/200 | Batch 50/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 174/200 | Batch 0/77 | train_loss: 0.073 | test_loss: 0.097\n",
      "Epoch 174/200 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.103\n",
      "Epoch 175/200 | Batch 0/77 | train_loss: 0.071 | test_loss: 0.097\n",
      "Epoch 175/200 | Batch 50/77 | train_loss: 0.070 | test_loss: 0.103\n",
      "Epoch 176/200 | Batch 0/77 | train_loss: 0.069 | test_loss: 0.095\n",
      "Epoch 176/200 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.102\n",
      "Epoch 177/200 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.096\n",
      "Epoch 177/200 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.104\n",
      "Epoch 178/200 | Batch 0/77 | train_loss: 0.069 | test_loss: 0.098\n",
      "Epoch 178/200 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.105\n",
      "Epoch 179/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.096\n",
      "Epoch 179/200 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.104\n",
      "Epoch 180/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.094\n",
      "Epoch 180/200 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.105\n",
      "Epoch 181/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.096\n",
      "Epoch 181/200 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.105\n",
      "Epoch 182/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.094\n",
      "Epoch 182/200 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.104\n",
      "Epoch 183/200 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.098\n",
      "Epoch 183/200 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.103\n",
      "Epoch 184/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.094\n",
      "Epoch 184/200 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.106\n",
      "Epoch 185/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.101\n",
      "Epoch 185/200 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.133\n",
      "Epoch 186/200 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.119\n",
      "Epoch 186/200 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.105\n",
      "Epoch 187/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.097\n",
      "Epoch 187/200 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.097\n",
      "Epoch 188/200 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.107\n",
      "Epoch 189/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.094\n",
      "Epoch 189/200 | Batch 50/77 | train_loss: 0.070 | test_loss: 0.107\n",
      "Epoch 190/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.099\n",
      "Epoch 190/200 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.108\n",
      "Epoch 191/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.095\n",
      "Epoch 191/200 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.108\n",
      "Epoch 192/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.095\n",
      "Epoch 192/200 | Batch 50/77 | train_loss: 0.065 | test_loss: 0.106\n",
      "Epoch 193/200 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.097\n",
      "Epoch 193/200 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.104\n",
      "Epoch 194/200 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.096\n",
      "Epoch 194/200 | Batch 50/77 | train_loss: 0.064 | test_loss: 0.104\n",
      "Epoch 195/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.099\n",
      "Epoch 195/200 | Batch 50/77 | train_loss: 0.064 | test_loss: 0.102\n",
      "Epoch 196/200 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.099\n",
      "Epoch 196/200 | Batch 50/77 | train_loss: 0.063 | test_loss: 0.101\n",
      "Epoch 197/200 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.096\n",
      "Epoch 197/200 | Batch 50/77 | train_loss: 0.063 | test_loss: 0.107\n",
      "Epoch 198/200 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.100\n",
      "Epoch 198/200 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.106\n",
      "Epoch 199/200 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.097\n",
      "Epoch 199/200 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.105\n",
      "Epoch 200/200 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.132\n",
      "Epoch 200/200 | Batch 50/77 | train_loss: 0.063 | test_loss: 0.107\n",
      "\n",
      "Source\n",
      "IN: c o m m o n <EOS>\n",
      "\n",
      "Target\n",
      "OUT: c m m n o o <EOS>\n",
      "\n",
      "Source\n",
      "IN: a p p l e <EOS>\n",
      "\n",
      "Target\n",
      "OUT: a e l p p <EOS>\n",
      "\n",
      "Source\n",
      "IN: z h e d o n g <EOS>\n",
      "\n",
      "Target\n",
      "OUT: d e g h n o z <EOS>\n"
     ]
    }
   ],
   "source": [
    "from pointer_net import PointerNetwork\n",
    "import sys\n",
    "import numpy as np\n",
    "if int(sys.version[0]) == 2:\n",
    "    from io import open\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "# end function\n",
    "\n",
    "\n",
    "def build_map(data):\n",
    "    specials = ['<GO>',  '<EOS>', '<PAD>', '<UNK>']\n",
    "    chars = list(set([char for line in data.split('\\n') for char in line]))\n",
    "    chars = sorted(chars)\n",
    "    idx2char = {idx: char for idx, char in enumerate(specials+chars)}\n",
    "    char2idx = {char: idx for idx, char in idx2char.items()}\n",
    "    return idx2char, char2idx\n",
    "# end function\n",
    "\n",
    "\n",
    "def preprocess_data(max_len):\n",
    "    X_data = read_data('temp/letters_source.txt')\n",
    "    Y_data = read_data('temp/letters_target.txt')\n",
    "\n",
    "    X_idx2char, X_char2idx = build_map(X_data)\n",
    "    print(\"==> Word Index Built\")\n",
    "\n",
    "    x_unk = X_char2idx['<UNK>']\n",
    "    x_eos = X_char2idx['<EOS>']\n",
    "    x_pad = X_char2idx['<PAD>']\n",
    "\n",
    "    X_indices = []\n",
    "    X_seq_len = []\n",
    "    Y_indices = []\n",
    "    Y_seq_len = []\n",
    "\n",
    "    for x_line, y_line in zip(X_data.split('\\n'), Y_data.split('\\n')):\n",
    "        x_chars = [X_char2idx.get(char, x_unk) for char in x_line]\n",
    "        _x_chars = x_chars + [x_eos] + [x_pad]* (max_len-1-len(x_chars))\n",
    "        \n",
    "        y_chars = [X_char2idx.get(char, x_unk) for char in y_line]\n",
    "        _y_chars = y_chars + [x_eos] + [x_pad]* (max_len-1-len(y_chars))\n",
    "        target = [_x_chars.index(y) for y in _y_chars] # we are predicting the positions\n",
    "\n",
    "        X_indices.append(_x_chars)\n",
    "        Y_indices.append(target)\n",
    "        X_seq_len.append(len(x_chars)+1)\n",
    "        Y_seq_len.append(len(y_chars)+1)\n",
    "\n",
    "    X_indices = np.array(X_indices)\n",
    "    Y_indices = np.array(Y_indices)\n",
    "    X_seq_len = np.array(X_seq_len)\n",
    "    Y_seq_len = np.array(Y_seq_len)\n",
    "    print(\"==> Sequence Padded\")\n",
    "\n",
    "    return X_indices, X_seq_len, Y_indices, Y_seq_len, X_char2idx, X_idx2char\n",
    "# end function\n",
    "\n",
    "\n",
    "def train_test_split(X_indices, X_seq_len, Y_indices, Y_seq_len, BATCH_SIZE):\n",
    "    X_train = X_indices[BATCH_SIZE:]\n",
    "    X_train_len = X_seq_len[BATCH_SIZE:]\n",
    "    Y_train = Y_indices[BATCH_SIZE:]\n",
    "    Y_train_len = Y_seq_len[BATCH_SIZE:]\n",
    "\n",
    "    X_test = X_indices[:BATCH_SIZE]\n",
    "    X_test_len = X_seq_len[:BATCH_SIZE]\n",
    "    Y_test = Y_indices[:BATCH_SIZE]\n",
    "    Y_test_len = Y_seq_len[:BATCH_SIZE]\n",
    "\n",
    "    return (X_train, X_train_len, Y_train, Y_train_len), (X_test, X_test_len, Y_test, Y_test_len)\n",
    "# end function\n",
    "\n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_LEN = 15\n",
    "    X_indices, X_seq_len, Y_indices, Y_seq_len, X_char2idx, X_idx2char = preprocess_data(MAX_LEN)\n",
    "    \n",
    "    (X_train, X_train_len, Y_train, Y_train_len), (X_test, X_test_len, Y_test, Y_test_len) \\\n",
    "        = train_test_split(X_indices, X_seq_len, Y_indices, Y_seq_len, BATCH_SIZE)\n",
    "    \n",
    "    model = PointerNetwork(\n",
    "        max_len = MAX_LEN,\n",
    "        rnn_size = 50,\n",
    "        X_word2idx = X_char2idx,\n",
    "        embedding_dim = 15)\n",
    "    \n",
    "    model.fit(X_train, X_train_len, Y_train, Y_train_len,\n",
    "        val_data=(X_test, X_test_len, Y_test, Y_test_len), batch_size=BATCH_SIZE, n_epoch=200)\n",
    "    model.infer('common', X_idx2char)\n",
    "    model.infer('apple', X_idx2char)\n",
    "    model.infer('zhedong', X_idx2char)\n",
    "# end main\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
